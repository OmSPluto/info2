<!DOCTYPE html>
<html>
<head>
    <title>Python Code Examples</title>
    <style>
        /* Add some basic styling for code */
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            font-family: monospace;
            white-space: pre-wrap; /* Preserve line breaks */
        }
        h2 {
            font-family: Arial, sans-serif;
        }
    </style>
</head>
<body>

<h2>#4_spam_classifier</h2>
<pre><code>
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

data = pd.read_csv("C:\\Users\\ompan\\OneDrive\\Desktop\\Practical Sem2\\Web Mining\\mail_data.csv")
data.head()

encoder = LabelEncoder()
data['Category'] = encoder.fit_transform(data['Category'])
data.head()

x = data['Message']
y = data['Category']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

tfidf = TfidfVectorizer(min_df=1, stop_words='english', lowercase=True)
x_train = tfidf.fit_transform(x_train)
x_test = tfidf.transform(x_test)

model = LogisticRegression()
model.fit(x_train, y_train)

pred = model.predict(x_test)
accuracy_score(y_test, pred)
</code></pre>

<h2>#5_web_crawler</h2>
<pre><code>
import requests
from bs4 import BeautifulSoup

code = requests.get("https://www.amazon.in")
plain = code.text

soup = BeautifulSoup(plain, "html.parser")
for link in soup.find_all('a'):
    print(link.get('href'))
</code></pre>

<h2>#6_text_mining</h2>
<pre><code>
import nltk
from nltk.probability import FreqDist
from nltk.tokenize import word_tokenize
nltk.download('punkt')

text = "In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the easternside of South America"

token = word_tokenize(text)
token

fdist = FreqDist(token)
fdist

fdist1 = fdist.most_common(10)
fdist1
</code></pre>

<h2>#7_stemming</h2>
<pre><code>
import nltk
nltk.download('wordnet')
from nltk.stem import PorterStemmer

pst = PorterStemmer()
pst.stem("waiting")

print('-'*20)
print("PORTER STEMMER")
print('-'*20)

stm = ["Waiting", "Waited", "Waits"]
for word in stm:
    print(word + ":" + pst.stem(word))

print('-'*20)
print("LANCASTER STEMMER")
print('-'*20)

from nltk.stem import LancasterStemmer
lst = LancasterStemmer()
stm = ["giving", "given", "gave"]
for word in stm:
    print(word + ":" + lst.stem(word))

print('-'*20)
print("WORD NET LEMMATIZER")
print('-'*20)

from nltk.stem import WordNetLemmatizer
lemma = WordNetLemmatizer()

print("rocks:", lemma.lemmatize("rocks"))
print("corpora:", lemma.lemmatize("corpora"))
</code></pre>

<h2>#8_stop_words</h2>
<pre><code>
import nltk
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords

a = set(stopwords.words('english'))

text = "Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal"
text1 = word_tokenize(text)

print('-'*15)
print("TOKENS")
print('-'*15)
print(text1)

stopwords = [x for x in text1 if x not in a]

print('-'*15)
print("STOPWORDS")
print('-'*15)
print(stopwords)

text = "vote to choose a particular man or a group (party) to represent them in parliament"
tex = word_tokenize(text)

print('-'*15)
print("POS TAG")
print('-'*15)
for token in tex:
    print(nltk.pos_tag([token]))
</code></pre>

</body>
</html>
